{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "from test import eval\n",
    "from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1024/198371: [>...............................] - ETA 0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 1.2sss\n",
      "Training DAGMM... Epoch: 0, Loss: 43065038.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 43097073.753\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 43083459.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 43092447.732\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 4, Loss: 43082362.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 43072141.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 43085888.639\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 7, Loss: 43078235.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 8, Loss: 43077461.897\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 9, Loss: 43084908.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 43068323.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 11, Loss: 43106304.041\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 43076459.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 13, Loss: 43081505.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 43088439.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 43070971.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 43065649.526\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 17, Loss: 43083100.598\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 18, Loss: 43069283.175\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 19, Loss: 43075103.320\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 20, Loss: 43080082.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 43076174.887\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 22, Loss: 43088163.196\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 23, Loss: 43078780.619\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 24, Loss: 43084651.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 25, Loss: 43072749.897\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 26, Loss: 43080775.155\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 27, Loss: 43085080.990\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 28, Loss: 43076550.062\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 29, Loss: 43076109.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 43094993.258\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 31, Loss: 43080644.041\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 32, Loss: 43100788.144\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 33, Loss: 43084281.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 43077232.701\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 35, Loss: 43085815.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 36, Loss: 43086250.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 43088283.979\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 43074096.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 43074791.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 40, Loss: 43095966.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 43101322.536\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 43103282.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 43102434.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 43069060.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 45, Loss: 43074877.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 43081692.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 43085332.990\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 48, Loss: 43077506.701\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 49, Loss: 43078397.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 50, Loss: 43095240.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 51, Loss: 43088525.773\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 52, Loss: 43081633.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 53, Loss: 43084432.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 43077313.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 55, Loss: 43062447.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 56, Loss: 43082435.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 43080778.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 43096253.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 43082393.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 43066855.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 43082993.773\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 62, Loss: 43069702.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 63, Loss: 43095562.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 43085220.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 65, Loss: 43095874.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 43068457.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 67, Loss: 43063635.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 68, Loss: 43101546.598\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 69, Loss: 43079401.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 70, Loss: 43095334.680\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 71, Loss: 43069860.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 43064034.742\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 73, Loss: 43062107.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 43090735.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 43073175.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 43081122.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 77, Loss: 43087262.392\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 78, Loss: 43067343.423\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 79, Loss: 43103929.918\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 80, Loss: 43089660.371\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 81, Loss: 43080217.258\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 82, Loss: 43090317.237\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 83, Loss: 43078276.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 84, Loss: 43080412.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 85, Loss: 43082480.907\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 86, Loss: 43081265.155\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 87, Loss: 43069174.186\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 88, Loss: 43070057.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 89, Loss: 43082439.340\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 90, Loss: 43075413.216\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 91, Loss: 43090430.206\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 92, Loss: 43073045.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 93, Loss: 43076483.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 43079612.247\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 95, Loss: 43063771.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 43102942.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 97, Loss: 43077618.309\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 98, Loss: 43094660.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 43079660.598\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 100, Loss: 43082306.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 43083539.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 43078700.536\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 103, Loss: 43106430.041\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 104, Loss: 43076232.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 105, Loss: 43087332.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 43070379.938\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 107, Loss: 43072816.928\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 108, Loss: 43079754.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 109, Loss: 43090714.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 110, Loss: 43067471.485\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 111, Loss: 43089620.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 43091823.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 43073657.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 43091895.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 43086023.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 43083342.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 117, Loss: 43078794.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 43067318.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 119, Loss: 43079388.082\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 120, Loss: 43063478.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 121, Loss: 43088449.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 43072771.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 43068286.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 124, Loss: 43080752.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 125, Loss: 43085541.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 43064572.536\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 127, Loss: 43082481.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 43080291.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 129, Loss: 43085915.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 130, Loss: 43076429.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 131, Loss: 43057598.021\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 132, Loss: 43077852.866\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 133, Loss: 43084148.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 43073377.423\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 135, Loss: 43081267.567\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 136, Loss: 43084593.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 43088257.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 138, Loss: 43073273.629\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 139, Loss: 43074540.062\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 140, Loss: 43071295.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 43080391.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 43070808.289\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 143, Loss: 43086532.082\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 144, Loss: 43075820.680\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 145, Loss: 43075231.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 43084941.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 147, Loss: 43074511.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 148, Loss: 43079230.330\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 149, Loss: 43075810.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 150, Loss: 43075883.320\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 151, Loss: 43090732.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 152, Loss: 43079805.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 153, Loss: 43081801.773\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 154, Loss: 43071096.763\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 155, Loss: 43078624.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 156, Loss: 43072408.371\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 157, Loss: 43080850.144\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 158, Loss: 43088534.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 159, Loss: 43095102.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 160, Loss: 43076601.629\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 161, Loss: 43086146.577\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 162, Loss: 43092120.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 163, Loss: 43073036.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 43081610.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 43074607.381\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 166, Loss: 43091955.258\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 43063851.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 43062664.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 43084274.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 43082829.237\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 43074880.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 43079505.052\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 173, Loss: 43090372.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 43073833.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 175, Loss: 43066475.732\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 176, Loss: 43061693.794\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 177, Loss: 43083229.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 43084695.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 43075621.464\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 180, Loss: 43077137.773\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 181, Loss: 43082469.216\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 182, Loss: 43081204.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 183, Loss: 43075331.340\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 184, Loss: 43085729.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 185, Loss: 43074187.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 43082189.835\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 187, Loss: 43095910.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 43086527.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 189, Loss: 43080836.763\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 190, Loss: 43090150.124\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 191, Loss: 43073106.825\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 192, Loss: 43092514.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 43098151.794\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 194, Loss: 43066148.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 43091540.639\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 196, Loss: 43086788.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 43085903.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 198, Loss: 43081863.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 43098706.041\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9486, Recall : 0.9169, F-score : 0.9325\n",
      "ROC AUC score: 98.00\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
